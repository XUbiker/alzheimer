{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to generate data lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_augm_set(dirs_with_labels, new_size, max_augm_params):\n",
    "    import numpy.random as rnd\n",
    "    import math\n",
    "    from augmentation import AugmParams\n",
    "    from xsets import XSet, XSetItem\n",
    "    \n",
    "    xset = XSet()\n",
    "    \n",
    "    if new_size == None or len(dirs_with_labels) == new_size:\n",
    "        for d in dirs_with_labels:\n",
    "            xset.add(XSetItem(label=d[0], image_dirs=(d[1], d[2]), augm_params=AugmParams()))\n",
    "        return xset\n",
    "    \n",
    "    augm_coeff = int(math.floor(new_size / len(dirs_with_labels)))\n",
    "\n",
    "    i = 0\n",
    "    for d in dirs_with_labels:\n",
    "        xset.add(XSetItem(label=d[0], image_dirs=(d[1], d[2]), augm_params=AugmParams()))\n",
    "        i += 1\n",
    "        for _ in range(augm_coeff-1):\n",
    "            p = AugmParams.trunc_random(max_augm_params)\n",
    "            xset.add(XSetItem(label=d[0], image_dirs=(d[1], d[2]), augm_params=p))\n",
    "            i += 1\n",
    "    while i < new_size:\n",
    "        ridx = rnd.randint(len(dirs_with_labels))\n",
    "        d = dirs_with_labels[ridx]\n",
    "        p = AugmParams.trunc_random(max_augm_params)\n",
    "        xset.add(XSetItem(label=d[0], image_dirs=(d[1], d[2]), augm_params=p))\n",
    "        i += 1\n",
    "    return xset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_samples_from_adni2(adni_root, max_augm_params, augm_factor, prefix_name='alz', valid_prc = 0.25, test_prc = 0.25, shuffle_data=True, debug=True):\n",
    "    \n",
    "    import os\n",
    "    import augmentation as augm\n",
    "    import numpy.random as rnd\n",
    "    from xsets import XSet\n",
    "\n",
    "    stage_dirs = {\n",
    "        'AD': '/AD/',\n",
    "        'MCI': '/MCI/',\n",
    "        'NC': '/NC/'\n",
    "    }\n",
    "\n",
    "    stage_dirs_root = {k: adni_root + v for k, v in stage_dirs.items()}\n",
    "    \n",
    "    patients_MRI_train = []\n",
    "    patients_MD_train = []\n",
    "    patients_MRI_test = []\n",
    "    patients_MD_test = []\n",
    "    \n",
    "    \n",
    "    class_size = {k: len(os.listdir(stage_dirs_root[k])) for k in stage_dirs_root}\n",
    "    print('source patients:', class_size)\n",
    "\n",
    "    ts = int(min(class_size.values()) * test_prc)\n",
    "    test_size = {k: ts for k in stage_dirs_root}\n",
    "    valid_size = {k: int(class_size[k] * valid_prc) for k in stage_dirs_root}\n",
    "    train_size = {k: class_size[k] - test_size[k] - valid_size[k] for k in stage_dirs_root}\n",
    "    \n",
    "    print('source patients used for train:', train_size)\n",
    "    print('source patients used for validation:', valid_size)\n",
    "    print('source patients used for test', test_size)\n",
    "\n",
    "    train_size_balanced = int(max(train_size.values()) * augm_factor)\n",
    "    valid_size_balanced = int(max(valid_size.values()) * augm_factor)\n",
    "    print('train data will be augmented to %d samples by each class' % train_size_balanced)\n",
    "    print('validation data will be augmented to %d samples by each class' % valid_size_balanced)\n",
    "    print('test data will be augmented to %d samples by each class' % ts)\n",
    "    \n",
    "    train_set = XSet(name = prefix_name + '_train')\n",
    "    valid_set = XSet(name = prefix_name + '_valid')\n",
    "    test_set = XSet(name = prefix_name + '_test')\n",
    "    \n",
    "    for k in stage_dirs_root:\n",
    "        stage_dir = stage_dirs[k]\n",
    "        patient_dirs = os.listdir(stage_dirs_root[k])\n",
    "        rnd.shuffle(patient_dirs)\n",
    "\n",
    "        test_dirs = patient_dirs[:test_size[k]]\n",
    "        valid_dirs = patient_dirs[test_size[k]:test_size[k]+valid_size[k]]\n",
    "        train_dirs = patient_dirs[test_size[k]+valid_size[k]:]\n",
    "                                 \n",
    "        train_lists = [(k, stage_dir + d + '/SMRI/', stage_dir + d + '/MD/') for d in train_dirs]\n",
    "        valid_lists = [(k, stage_dir + d + '/SMRI/', stage_dir + d + '/MD/') for d in valid_dirs]\n",
    "        test_lists = [(k, stage_dir + d + '/SMRI/', stage_dir + d + '/MD/') for d in test_dirs]\n",
    "        \n",
    "        train_set.add_all(generate_augm_set(train_lists, train_size_balanced, max_augm_params))\n",
    "        valid_set.add_all(generate_augm_set(valid_lists, valid_size_balanced, max_augm_params))\n",
    "        test_set.add_all(generate_augm_set(test_lists, None, None))\n",
    "    \n",
    "    if shuffle_data:\n",
    "        train_set.shuffle()\n",
    "        valid_set.shuffle()\n",
    "        test_set.shuffle()\n",
    "    \n",
    "    if debug:\n",
    "        train_set.print()\n",
    "        valid_set.print()\n",
    "        test_set.print()\n",
    "        \n",
    "        \n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of how to do data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import augmentation as augm\n",
    "\n",
    "lists_params = {\n",
    "    'adni_root': 'C:/dev/ADNI_Multimodal/dataset/',\n",
    "    # 'adni_root': '/home/xubiker/ADNI_Multimodal/dataset/',\n",
    "    'prefix_name': 'alz',\n",
    "    'max_augm': augm.AugmParams(shift=(2, 2, 2), sigma=1.2),\n",
    "    'test_prc': 0.25,\n",
    "    'valid_prc': 0.25,\n",
    "    'augm_factor': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_params(params, file_path):\n",
    "    import pickle\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate lists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_sets(lists_file_path, params, debug=True):\n",
    "    import ex_utils\n",
    "    train_set, valid_set, test_set = generate_samples_from_adni2(\n",
    "        params['adni_root'],\n",
    "        params['max_augm'], test_prc=params['test_prc'], valid_prc=params['valid_prc'],\n",
    "        augm_factor=params['augm_factor'],\n",
    "        prefix_name=params['prefix_name'],\n",
    "        shuffle_data=False, debug=debug\n",
    "    )\n",
    "    ex_utils.save_pickle((train_set, valid_set, test_set), lists_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ex_utils\n",
    "ex_utils.save_pickle(lists_params, 'params.pkl')\n",
    "generate_sets('sets.pkl', lists_params, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
