{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All settings and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = {\n",
    "    'caffe_root': '/home/xubiker/dev/caffe_official/',\n",
    "    'C3D_root': '/home/xubiker/dev/C3D-v1.1/',\n",
    "    'caffe_dev_root': '/home/xubiker/dev/caffe/',\n",
    "    'caffe_dev_modified_root': '/home/xubiker/dev/caffe_modified/',\n",
    "    'DTYPE': np.float32,\n",
    "    'ADNI2_DIR': '/home/xubiker/ADNI2/dataset/',\n",
    "    'ADNI2_MODIFIED_DIR': '/home/xubiker/ADNI2_modified/dataset/',\n",
    "    'DATA_EXT': '.nii',\n",
    "    'TF_DATA_DIR': '/home/xubiker/dev/alzheimer/alzheimer_tf/data',\n",
    "    'TF_DATA_EXT': '.tfrecords',\n",
    "    'DATA_SPLIT_PRC': 0.6,\n",
    "    'DO_CROP': True,\n",
    "    'CROP_PRC': (0.05, 0.05, 0.05),\n",
    "    'SHIFT_PRC': (0, 0, -0.05),\n",
    "    'LMDB_SHUFFLE': True,\n",
    "    'LMDB_SHUFFLE_SAME': True,\n",
    "    'LMDB_PREFIX': 'alz'\n",
    "}\n",
    "\n",
    "p['IDIMS'] = [109, 131, 109] if p['DO_CROP'] else [121, 145, 121]\n",
    "p['IDIMSFLT'] = p['IDIMS'][0] * p['IDIMS'][1] * p['IDIMS'][2]\n",
    "\n",
    "label_code = {\n",
    "    'AD': 2,\n",
    "    'MCI': 1,\n",
    "    'NC': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to load caffe\n",
    "def load_caffe(caffe_version):\n",
    "    \"\"\"\n",
    "    caffe_version:\n",
    "    0 - official caffe,\n",
    "    1 - C3D from facebook,\n",
    "    2 - dev caffe,\n",
    "    3 - modified dev caffe\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    root = {\n",
    "        0: p['caffe_root'],\n",
    "        1: p['C3D_root'],\n",
    "        2: p['caffe_dev_root'],\n",
    "        3: p['caffe_dev_modified_root']\n",
    "    }[caffe_version]\n",
    "    pcr = root + \"/python\"\n",
    "    if not pcr in sys.path:\n",
    "        sys.path.append(pcr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_caffe(3)\n",
    "import caffe\n",
    "print('caffe', caffe.__version__, 'loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LMDB from ADNI2 **[uncomment to use]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# augmentation = {'n': 1, 'max_shift': 2, 'max_blur': 7}\n",
    "# create_lmdb_from_adni2(p['ADNI2_MODIFIED_DIR'], p['LMDB_PREFIX'], augm_params = None, crop_prc=p['CROP_PRC'], shift_prc=p['SHIFT_PRC'])\n",
    "# validate_lmdb_from_adni2(p['LMDB_PREFIX'], preview_only=False)\n",
    "\n",
    "max_augm = {'shift': 2, 'blur': 1.2}\n",
    "(train_lists, valid_lists, test_lists) = generate_lists_from_adni2(adni_root=p['ADNI2_MODIFIED_DIR'], max_augm_params=max_augm, augm_factor=5, shuffle_data=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean images for training databases and store them in binaryproto format **[uncomment to use]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean_MRI = calc_lmdb_mean(p['LMDB_PREFIX'] + '_MRI_train', reshape_4D=True)\n",
    "# mean_MD = calc_lmdb_mean(p['LMDB_PREFIX'] + '_MD_train', reshape_4D=True)\n",
    "# array_to_proto(mean_MRI, p['LMDB_PREFIX'] + '_MRI_mean.binaryproto')\n",
    "# array_to_proto(mean_MD, p['LMDB_PREFIX'] + '_MD_mean.binaryproto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_net_path     = p['LMDB_PREFIX'] + '_train.prototxt'\n",
    "test_net_path      = p['LMDB_PREFIX'] + '_test.prototxt'\n",
    "solver_config_path = p['LMDB_PREFIX'] + '_solver.prototxt'\n",
    "mean_MRI           = p['LMDB_PREFIX'] + '_MRI_mean.binaryproto'\n",
    "mean_MD            = p['LMDB_PREFIX'] + '_MD_mean.binaryproto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create custom net and solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_net_straight(mri_lmdb, batch_size):\n",
    "\n",
    "    import caffe\n",
    "    from caffe import layers as L, params as P\n",
    "    \n",
    "    mri_train_data_lmdb = p['LMDB_PREFIX'] + '_MRI_train'\n",
    "    \n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    sc = 1.0 / 255.0\n",
    "\n",
    "    pool_type = P.Pooling.MAX #P.Pooling.AVE\n",
    "    filler_type = dict(type='gaussian')#dict(type='xavier')\n",
    "    \n",
    "    # --- data layers ---\n",
    "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=mri_lmdb, transform_param={'scale':sc, 'mean_file':mean_MRI}, ntop=2)\n",
    "    n.resh1  = L.Reshape(n.data, reshape_param={'shape':{'dim': [batch_size, 1]+p['IDIMS']}})\n",
    "\n",
    "    # --- first group of convolutional layers ---\n",
    "    n.conv1 = L.Convolution(n.resh1, kernel_size=7, num_output=8, weight_filler=filler_type)\n",
    "    n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu1 = L.ReLU(n.pool1)\n",
    "    \n",
    "    # --- second group of convolutional layers ---\n",
    "    n.conv2 = L.Convolution(n.relu1, kernel_size=5, num_output=16, weight_filler=filler_type)\n",
    "    n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu2 = L.ReLU(n.pool2)\n",
    "    \n",
    "    # --- third group of convolutional layers ---\n",
    "    n.conv3 = L.Convolution(n.relu2, kernel_size=3, num_output=32, weight_filler=filler_type)\n",
    "    n.pool3 = L.Pooling(n.conv3, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu3 = L.ReLU(n.pool3)\n",
    "    \n",
    "    # --- fourth group of convolutional layers ---\n",
    "    n.conv4 = L.Convolution(n.relu3, kernel_size=3, num_output=64, weight_filler=filler_type)\n",
    "    n.pool4 = L.Pooling(n.conv4, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu4 = L.ReLU(n.pool4)\n",
    "    \n",
    "    # --- first fully connected layer ---\n",
    "    n.fc1      = L.InnerProduct(n.relu4, num_output=64)\n",
    "    n.fcrelu1  = L.ReLU(n.fc1)\n",
    "    \n",
    "#     # --- second fully connected layer ---\n",
    "#     n.fc2      = L.InnerProduct(n.fcrelu1, num_output=16)\n",
    "#     n.fcrelu2  = L.ReLU(n.fc2)\n",
    "\n",
    "    n.score    = L.InnerProduct(n.fcrelu1, num_output=3)\n",
    "    n.loss     = L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_net_siamese(mri_lmdb, md_lmdb, batch_size):\n",
    "\n",
    "    import caffe\n",
    "    from caffe import layers as L, params as P\n",
    "    \n",
    "    mri_train_data_lmdb = p['LMDB_PREFIX'] + '_MRI_train'\n",
    "    md_train_data_lmdb = p['LMDB_PREFIX'] + '_MD_train'\n",
    "    \n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    sc = 1.0 / 255.0\n",
    "\n",
    "    ## indicate engines if needed\n",
    "    ## engine for convolution: engine=P.Convolution.CAFFE, p.Convolution.CUDNN\n",
    "    ## engine for pooling: engine=P.Pooling.CAFFE, P.Pooling.CUDNN\n",
    "    \n",
    "    pool_type = P.Pooling.MAX #P.Pooling.AVE\n",
    "    \n",
    "    # --- data layers ---\n",
    "    n.data_MRI, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=mri_lmdb, transform_param={'scale':sc, 'mean_file':mean_MRI}, ntop=2)\n",
    "    n.data_MD = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=md_lmdb, transform_param={'scale':sc, 'mean_file':mean_MD}, ntop=1)\n",
    "    n.resh1_MRI  = L.Reshape(n.data_MRI, reshape_param={'shape':{'dim': [batch_size, 1]+p['IDIMS']}})\n",
    "    n.resh1_MD   = L.Reshape(n.data_MD, reshape_param={'shape':{'dim': [batch_size, 1]+p['IDIMS']}})\n",
    "\n",
    "    # --- first group of convolutional layers ---\n",
    "    n.conv1_MRI = L.Convolution(n.resh1_MRI, kernel_size=7, num_output=8, weight_filler=dict(type='xavier'))\n",
    "    n.pool1_MRI = L.Pooling(n.conv1_MRI, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu1_MRI = L.ReLU(n.pool1_MRI)\n",
    "\n",
    "    n.conv1_MD = L.Convolution(n.resh1_MD, kernel_size=7, num_output=8, weight_filler=dict(type='xavier'))\n",
    "    n.pool1_MD = L.Pooling(n.conv1_MD, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu1_MD = L.ReLU(n.pool1_MD)\n",
    "    \n",
    "    # --- second group of convolutional layers ---\n",
    "    n.conv2_MRI = L.Convolution(n.relu1_MRI, kernel_size=5, num_output=16, weight_filler=dict(type='xavier'))\n",
    "    n.pool2_MRI = L.Pooling(n.conv2_MRI, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu2_MRI = L.ReLU(n.pool2_MRI)\n",
    "\n",
    "    n.conv2_MD = L.Convolution(n.relu1_MD, kernel_size=5, num_output=16, weight_filler=dict(type='xavier'))\n",
    "    n.pool2_MD = L.Pooling(n.conv2_MD, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu2_MD = L.ReLU(n.pool2_MD)\n",
    "    \n",
    "    # --- third group of convolutional layers ---\n",
    "    n.conv3_MRI = L.Convolution(n.relu2_MRI, kernel_size=3, num_output=32, weight_filler=dict(type='xavier'))\n",
    "    n.pool3_MRI = L.Pooling(n.conv3_MRI, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu3_MRI = L.ReLU(n.pool3_MRI)\n",
    "\n",
    "    n.conv3_MD = L.Convolution(n.relu2_MD, kernel_size=3, num_output=32, weight_filler=dict(type='xavier'))\n",
    "    n.pool3_MD = L.Pooling(n.conv3_MD, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu3_MD = L.ReLU(n.pool3_MD)\n",
    "    \n",
    "    # --- fourth group of convolutional layers ---\n",
    "    n.conv4_MRI = L.Convolution(n.relu3_MRI, kernel_size=3, num_output=64, weight_filler=dict(type='xavier'))\n",
    "    n.pool4_MRI = L.Pooling(n.conv4_MRI, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu4_MRI = L.ReLU(n.pool4_MRI)\n",
    "\n",
    "    n.conv4_MD = L.Convolution(n.relu3_MD, kernel_size=3, num_output=64, weight_filler=dict(type='xavier'))\n",
    "    n.pool4_MD = L.Pooling(n.conv4_MD, kernel_size=2, stride=2, pool=pool_type)\n",
    "    n.relu4_MD = L.ReLU(n.pool4_MD)\n",
    "    \n",
    "    # --- concatenation layer ---\n",
    "    n.join = L.Concat(n.relu4_MRI, n.relu4_MD)\n",
    "    \n",
    "    # --- first fully connected layer ---\n",
    "    n.fc1      = L.InnerProduct(n.join, num_output=64)\n",
    "    n.fcrelu1  = L.ReLU(n.fc1)\n",
    "    \n",
    "    # --- second fully connected layer ---\n",
    "    n.fc2      = L.InnerProduct(n.fcrelu1, num_output=16)\n",
    "    n.fcrelu2  = L.ReLU(n.fc2)\n",
    "\n",
    "    n.score    = L.InnerProduct(n.fcrelu2, num_output=3)\n",
    "    n.loss     = L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_solver(train_net_path, test_net_path):\n",
    "\n",
    "    import caffe\n",
    "    from caffe.proto import caffe_pb2\n",
    "    \n",
    "    s = caffe_pb2.SolverParameter()\n",
    "\n",
    "    s.random_seed = 0xCAFFE # Set a seed for reproducible experiments\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    s.test_net.append(test_net_path)\n",
    "    s.test_interval = 10  # Test after every _ training iterations.\n",
    "    s.test_iter.append(100) # Test on _ batches each time we test.\n",
    "\n",
    "    s.max_iter = 1000     # no. of times to update the net (training iterations)\n",
    "\n",
    "    s.type = \"Nesterov\" # Adam\", \"Nesterov\", ...\n",
    "\n",
    "    s.base_lr = 0.1 # Set the initial learning rate\n",
    "    s.momentum = 0.9 # Set momentum to accelerate learning\n",
    "    s.weight_decay = 5e-4 # Set weight decay to regularize and prevent overfitting\n",
    "\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    s.lr_policy = 'inv'\n",
    "    s.gamma = 0.1\n",
    "    s.power = 0.5\n",
    "\n",
    "    # Display the current training loss and accuracy every _ iterations.\n",
    "    s.display = 10\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.\n",
    "    # We'll snapshot every _ iterations -- twice during training.\n",
    "    s.snapshot = 100\n",
    "    s.snapshot_prefix = 'alz_net'\n",
    "\n",
    "    # Train on the GPU\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the architecture of the net, we can check the dimensions of the intermediate features (blobs) and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_blobs_dimensions(solver):\n",
    "    print('The network is represented with the following blobs:')\n",
    "    info = [(k, v.data.shape) for k, v in solver.net.blobs.items()]\n",
    "    for (n, i) in info: print(n, '-', i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def configure_net_and_write_to_prototxt():\n",
    "    # --- create train and test nets and load the solver ---\n",
    "    pfx = p['LMDB_PREFIX']\n",
    "    # --- for siamese:\n",
    "#     with open(train_net_path, 'w') as f:\n",
    "#         f.write(str(custom_net_siamese(pfx + '_MRI_train', pfx + '_MD_train')))    \n",
    "#     with open(test_net_path, 'w') as f:\n",
    "#         f.write(str(custom_net_siamese(pfx + '_MRI_test', pfx + '_MD_test')))\n",
    "    # --- for straight:\n",
    "    with open(train_net_path, 'w') as f:\n",
    "        f.write(str(custom_net_straight(pfx + '_MRI_train', 7)))    \n",
    "    with open(test_net_path, 'w') as f:\n",
    "        f.write(str(custom_net_straight(pfx + '_MRI_test', 1)))\n",
    "    with open(solver_config_path, 'w') as f:\n",
    "        f.write(str(custom_solver(train_net_path, test_net_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    caffe.set_device(0)\n",
    "    caffe.set_mode_gpu()\n",
    "    solver = None\n",
    "    solver = caffe.get_solver(solver_config_path)\n",
    "    \n",
    "    check_blobs_dimensions(solver)\n",
    "\n",
    "    # -------------------- solve --------------------\n",
    "    print('Solving...')\n",
    "    \n",
    "    niter = 200\n",
    "    test_interval = 10\n",
    "    ntest = 100\n",
    "\n",
    "    loss = np.zeros(niter).astype(np.int)\n",
    "    acc = np.zeros(int(np.ceil(niter / test_interval)))\n",
    "\n",
    "    # the main solver loop\n",
    "    for it in range(niter):\n",
    "        solver.step(1)\n",
    "        loss[it] = solver.net.blobs['loss'].data\n",
    "        if it % test_interval == 0:\n",
    "            print('Iteration', it, 'testing...')\n",
    "            confusion = np.zeros((3, 3)).astype(np.int)\n",
    "            for test_it in range(ntest):\n",
    "                solver.test_nets[0].forward()\n",
    "                predicted_label = solver.test_nets[0].blobs['score'].data.argmax(1)[0]\n",
    "                real_label = int(solver.test_nets[0].blobs['label'].data[0])\n",
    "                confusion[predicted_label, real_label] += 1\n",
    "            print(confusion)\n",
    "            correct = np.trace(confusion)\n",
    "            moment_accuracy = correct / ntest\n",
    "            print('correct %d out of %d (%f)' % (correct, ntest, moment_accuracy))\n",
    "            acc[it // test_interval] = correct / ntest\n",
    "\n",
    "    _, ax1 = subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(arange(niter), loss)\n",
    "    ax2.plot(test_interval * arange(len(acc)), acc, 'r')\n",
    "    ax1.set_xlabel('iteration')\n",
    "    ax1.set_ylabel('train loss')\n",
    "    ax2.set_ylabel('test accuracy')\n",
    "    ax2.set_title('Custom Test Accuracy: {:.2f}'.format(acc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configure_net_and_write_to_prototxt()\n",
    "# run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
