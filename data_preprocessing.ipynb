{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some settings and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_code_ternary = {\n",
    "    'NC': 0,\n",
    "    'MCI': 1,\n",
    "    'AD': 2\n",
    "}\n",
    "\n",
    "label_code_binary = {\n",
    "    'AD-MCI': {\n",
    "        'MCI': 0,\n",
    "        'AD': 1\n",
    "    },\n",
    "    'MCI-AD': {\n",
    "        'MCI': 0,\n",
    "        'AD': 1\n",
    "    },\n",
    "    'MCI-NC': {\n",
    "        'NC': 0,\n",
    "        'MCI': 1\n",
    "    },\n",
    "    'NC-MCI': {\n",
    "        'NC': 0,\n",
    "        'MCI': 1\n",
    "    },\n",
    "    'AD-NC': {\n",
    "        'NC': 0,\n",
    "        'AD': 1\n",
    "    },\n",
    "    'NC-AD': {\n",
    "        'NC': 0,\n",
    "        'AD': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "caffe_folder = '/home/xubiker/dev/caffe_modified/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load caffe\n",
    "\n",
    "It is needed to work with lmdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_caffe(caffe_root):\n",
    "    import sys\n",
    "    pcr = caffe_root + \"/python\"\n",
    "    if not pcr in sys.path:\n",
    "        sys.path.append(pcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_caffe(caffe_folder)\n",
    "import caffe\n",
    "print('caffe', caffe.__version__, 'loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform .nii to np-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nii_to_array(nii_filename, data_type, fix_nan=True):\n",
    "    import os\n",
    "    import nibabel as nib\n",
    "    import numpy as np\n",
    "    img = nib.load(nii_filename)\n",
    "    np_data = img.get_data().astype(data_type)\n",
    "    if fix_nan:\n",
    "        np_data = np.nan_to_num(np_data)\n",
    "    return np_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to save data to LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initiate_lmdb(lmdb_name, drop_existing = False):\n",
    "    import lmdb\n",
    "    import caffe\n",
    "    import numpy as np\n",
    "    \n",
    "    if drop_existing:\n",
    "        import os\n",
    "        import shutil\n",
    "        if os.path.exists(lmdb_name):\n",
    "            shutil.rmtree(lmdb_name) \n",
    "    \n",
    "    env = lmdb.open(lmdb_name, map_size=int(1e12))\n",
    "    print('database debug info:', env.stat())\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_transaction(txn, data, label, key):\n",
    "    import caffe\n",
    "    datum = caffe.proto.caffe_pb2.Datum()\n",
    "    (datum.channels, datum.height, datum.width) = data.shape\n",
    "    datum.data = data.tobytes()\n",
    "    datum.label = label\n",
    "    key = '{:08}'.format(key)\n",
    "    txn.put(key.encode('ascii'), datum.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def array_to_proto(data, proto_name):\n",
    "    import caffe\n",
    "    blob = caffe.io.array_to_blobproto(data)\n",
    "    binaryproto_file = open(proto_name, 'wb+')\n",
    "    binaryproto_file.write(blob.SerializeToString())\n",
    "    binaryproto_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to validate LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debug_lmdb_print_info(lmdb_name):\n",
    "    import caffe\n",
    "    print('debug printing for \\'', lmdb_name, '\\' lmdb data')\n",
    "    env = initiate_lmdb(lmdb_name, drop_existing = False)\n",
    "    print(env.stat())\n",
    "    with env.begin() as txn:\n",
    "        cursor = txn.cursor()\n",
    "        datum = caffe.proto.caffe_pb2.Datum()\n",
    "        i = 0\n",
    "        for key, value in cursor:\n",
    "            i += 1\n",
    "            datum.ParseFromString(value)\n",
    "            print('inst %d of size (%d, %d, %d) labeled %d' % (i, datum.channels, datum.height, datum.width, datum.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debug_plot_median_slices(np_data, print_slices=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    x, y, z = np_data.shape\n",
    "    slc = np_data[:, :, z//2]\n",
    "    if print_slices: print(slc)\n",
    "    plt.matshow(slc, interpolation='nearest', cmap='gray')\n",
    "    plt.show()\n",
    "    slc = np_data[:, y//2, :]\n",
    "    if print_slices: print(slc)\n",
    "    plt.matshow(slc, interpolation='nearest', cmap='gray')\n",
    "    plt.show()\n",
    "    slc = np_data[x//2, :, :]\n",
    "    if print_slices: print(slc)\n",
    "    plt.matshow(slc, interpolation='nearest', cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debug_lmdb_plot_slices(lmdb_name, data_type, print_slices=False):\n",
    "    import numpy as np\n",
    "    #np.set_printoptions(threshold=np.inf)\n",
    "    import caffe\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('debug plotting slices for \\'%s\\' lmdb data' % lmdb_name)\n",
    "    env = initiate_lmdb(lmdb_name, drop_existing = False)\n",
    "    with env.begin() as txn:\n",
    "        cursor = txn.cursor()\n",
    "        datum = caffe.proto.caffe_pb2.Datum()\n",
    "        cursor.next();\n",
    "        value = cursor.value();\n",
    "        datum.ParseFromString(value)\n",
    "        flat_x = np.fromstring(datum.data, dtype=data_type)\n",
    "        x = flat_x.reshape(datum.channels, datum.height, datum.width)\n",
    "        debug_plot_median_slices(x, print_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to generate data lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_augm_params(max_augm_params):\n",
    "    import numpy.random as rnd\n",
    "    max_shift = max_augm_params['shift']\n",
    "    max_blur = max_augm_params['blur']\n",
    "    while True:\n",
    "        shift_x = rnd.randint(-max_shift, max_shift)\n",
    "        shift_y = rnd.randint(-max_shift, max_shift)\n",
    "        shift_z = rnd.randint(-max_shift, max_shift)\n",
    "        blur_sigma = float(rnd.randint(1000)) / 1000 * max_blur\n",
    "        if shift_x + shift_y + shift_z + blur_sigma > 0:\n",
    "            return (shift_x, shift_y, shift_z, blur_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_augm_lists(dirs_with_labels, new_size, max_augm_params, default_augm_params=None):\n",
    "    import numpy.random as rnd\n",
    "    import math\n",
    "    if new_size == None or len(dirs_with_labels) == new_size:\n",
    "        return [dwl + [default_augm_params] for dwl in dirs_with_labels]\n",
    "    augm_coeff = int(math.floor(new_size / len(dirs_with_labels)))\n",
    "    res = []\n",
    "    i = 0\n",
    "    for dwl in dirs_with_labels:\n",
    "        res.append(dwl + [(0, 0, 0, 0.0)])\n",
    "        i += 1\n",
    "        for _ in range(augm_coeff-1):\n",
    "            res.append(dwl + [generate_augm_params(max_augm_params)])\n",
    "            i += 1\n",
    "    while i < new_size:\n",
    "        ridx = rnd.randint(len(dirs_with_labels))\n",
    "        dwl = dirs_with_labels[ridx]\n",
    "        res.append(dwl +[generate_augm_params(max_augm_params)])\n",
    "        i += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_lists_from_adni2(adni_root, max_augm_params, augm_factor, valid_prc = 0.25, test_prc = 0.25, shuffle_data=True, debug=True):\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import numpy.random as rnd\n",
    "\n",
    "    stage_dirs = {\n",
    "        'AD': '/AD/',\n",
    "        'MCI': '/MCI/',\n",
    "        'NC': '/NC/'\n",
    "    }\n",
    "\n",
    "    stage_dirs_root = {k: adni_root + v for k, v in stage_dirs.items()}\n",
    "    \n",
    "    default_augm = (0, 0, 0, 0.0)\n",
    "    \n",
    "    patients_MRI_train = []\n",
    "    patients_MD_train = []\n",
    "    patients_MRI_test = []\n",
    "    patients_MD_test = []\n",
    "    \n",
    "    \n",
    "    class_size = {k: len(os.listdir(stage_dirs_root[k])) for k in stage_dirs_root}\n",
    "    print('source patients:', class_size)\n",
    "\n",
    "    ts = int(min(class_size.values()) * test_prc)\n",
    "    test_size = {k: ts for k in stage_dirs_root}\n",
    "    valid_size = {k: int(class_size[k] * valid_prc) for k in stage_dirs_root}\n",
    "    train_size = {k: class_size[k] - test_size[k] - valid_size[k] for k in stage_dirs_root}\n",
    "    \n",
    "    print('source patients used for train:', train_size)\n",
    "    print('source patients used for validation:', valid_size)\n",
    "    print('source patients used for test', test_size)\n",
    "\n",
    "    train_size_balanced = int(max(train_size.values()) * augm_factor)\n",
    "    valid_size_balanced = int(max(valid_size.values()) * augm_factor)\n",
    "    print('train data will be augmented to %d samples by each class' % train_size_balanced)\n",
    "    print('validation data will be augmented to %d samples by each class' % valid_size_balanced)\n",
    "    print('test data will be augmented to %d samples by each class' % ts)\n",
    "    \n",
    "    train_lists_out = []\n",
    "    valid_lists_out = []\n",
    "    test_lists_out = []\n",
    "    \n",
    "    for k in stage_dirs_root:\n",
    "        stage_dir = stage_dirs[k]\n",
    "        patient_dirs = os.listdir(stage_dirs_root[k])\n",
    "        rnd.shuffle(patient_dirs)\n",
    "\n",
    "        test_dirs = patient_dirs[:test_size[k]]\n",
    "        valid_dirs = patient_dirs[test_size[k]:test_size[k]+valid_size[k]]\n",
    "        train_dirs = patient_dirs[test_size[k]+valid_size[k]:]\n",
    "                                 \n",
    "        train_lists = [[k, stage_dir + d + '/SMRI/', stage_dir + d + '/MD/'] for d in train_dirs]\n",
    "        valid_lists = [[k, stage_dir + d + '/SMRI/', stage_dir + d + '/MD/'] for d in valid_dirs]\n",
    "        test_lists = [[k, stage_dir + d + '/SMRI/', stage_dir + d + '/MD/'] for d in test_dirs]\n",
    "        \n",
    "        train_lists_out += generate_augm_lists(train_lists, train_size_balanced, max_augm_params)\n",
    "        valid_lists_out += generate_augm_lists(valid_lists, valid_size_balanced, max_augm_params)\n",
    "        test_lists_out += generate_augm_lists(test_lists, None, None, default_augm_params=default_augm)\n",
    "    \n",
    "    if shuffle_data:\n",
    "        rnd.shuffle(train_lists_out)\n",
    "        rnd.shuffle(valid_lists_out)\n",
    "        rnd.shuffle(test_lists_out)\n",
    "    \n",
    "    if debug:\n",
    "        print('### train lists (%d instances):' % len(train_lists_out))\n",
    "        for i in train_lists_out: print(i)\n",
    "        print('### valid lists (%d instances):' % len(valid_lists_out))\n",
    "        for i in valid_lists_out: print(i)\n",
    "        print('### test lists (%d instances):' % len(test_lists_out))\n",
    "        for i in test_lists_out: print(i)\n",
    "        \n",
    "        \n",
    "    return (train_lists_out, valid_lists_out, test_lists_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_mri_dti(item_list):\n",
    "    mri_list = [(i[0], i[1], i[3]) for i in item_list]\n",
    "    dti_list = [(i[0], i[2], i[3]) for i in item_list]\n",
    "    return mri_list, dti_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nii_from_folder(folder):\n",
    "    import os\n",
    "    res = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.nii'):\n",
    "                res.append(os.path.join(root, file))\n",
    "    if len(res) > 1:\n",
    "        print('WARNING. Folder %s contains more than one files' % folder)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to preprocess and augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop(data, crop_prc, shift_prc):\n",
    "    dims = np.array(data.shape).astype(np.float)\n",
    "    pads = np.round(dims * np.array(crop_prc).astype(np.float)).astype(np.int)\n",
    "    shifts = np.round(dims * np.array(shift_prc).astype(np.float)).astype(np.int)\n",
    "    if pads.size != 3:\n",
    "        raise NameError('unsupported number of dimensions')\n",
    "    else:\n",
    "        x, y, z = data.shape\n",
    "        pad_x, pad_y, pad_z = pads\n",
    "        sh_x, sh_y, sh_z = shifts\n",
    "        data_new = data[sh_x+pad_x:x+sh_x-pad_x, sh_y+pad_y:y+sh_y-pad_y, sh_z+pad_z:z+sh_z-pad_z]\n",
    "        print('cropping data:', data.shape, '->', data_new.shape)\n",
    "        return data_new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augment(data, max_shift, augm_params):\n",
    "\n",
    "    # augm_params should be a tuple of 4 elements: shift_x, shift_y, shift_z, blur_sigma\n",
    "    if data.ndim != 3 or len(augm_params) != 4: raise NameError('invalid input')\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.ndimage.filters import gaussian_filter\n",
    "    \n",
    "    shift_x = augm_params[0]\n",
    "    shift_y = augm_params[1]\n",
    "    shift_z = augm_params[2]\n",
    "    blur_sigma = augm_params[3]\n",
    "    \n",
    "    s_x, s_y, s_z = (data.shape[0] - 2 * max_shift, data.shape[1] - 2 * max_shift, data.shape[2] - 2 * max_shift)\n",
    "\n",
    "    blurred = data if blur_sigma == 0 else gaussian_filter(data, sigma = blur_sigma)\n",
    "    sub_data = blurred[max_shift + shift_x : s_x + max_shift + shift_x,\n",
    "                       max_shift + shift_y : s_y + max_shift + shift_y,\n",
    "                       max_shift + shift_z : s_z + max_shift + shift_z]\n",
    "    return sub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(list_item, adni_root, data_type, max_augm_shift, crop_params=None, crop_roi_params=None):\n",
    "    nii = get_nii_from_folder(adni_root + list_item[1])[0]\n",
    "    array = nii_to_array(nii, data_type)\n",
    "    if crop_params != None:\n",
    "        array = crop(array, crop_prc=crop_params['prc'], shift_prc=crop_params['shift'])\n",
    "    augm = augment(array, max_augm_shift, list_item[2])\n",
    "    if crop_roi_params != None:\n",
    "        crp = crop_roi_params # (min_x, max_x, min_y, max_y, min_z, max_z)\n",
    "        augm = augm[crp[0]:crp[1], crp[2]:crp[3], crp[4]:crp[5]]\n",
    "    return augm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_lmdb(one_modality_list, adni_root, data_type, lmdb_name, label_code, max_augm_shift, crop_params=None, crop_roi_params=None):\n",
    "    env = initiate_lmdb(lmdb_name, drop_existing = True)\n",
    "    key = 0\n",
    "    with env.begin(write=True) as txn:\n",
    "        for i in one_modality_list:\n",
    "            augm = process(i, adni_root, data_type, max_augm_shift, crop_params, crop_roi_params)\n",
    "            print('%d. writing image of shape %s to lmdb (%s)' % (key, str(augm.shape), i[1]))\n",
    "            write_to_transaction(txn, augm, label_code[i[0]], key)\n",
    "            key += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_lists_to_binary_groups(lists):\n",
    "    lbls = list(label_code_ternary.keys())\n",
    "    bin_labels = {'01': lbls[0]+'-'+lbls[1], '12': lbls[1]+'-'+lbls[2], '02': lbls[0]+'-'+lbls[2]}\n",
    "    bin_groups = {'01': [], '12': [], '02': []}\n",
    "    for item in lists:\n",
    "        if item[0] == lbls[0]:\n",
    "            bin_groups['01'].append(item)\n",
    "            bin_groups['02'].append(item)\n",
    "        if item[0] == lbls[1]:\n",
    "            bin_groups['01'].append(item)\n",
    "            bin_groups['12'].append(item)\n",
    "        if item[0] == lbls[2]:\n",
    "            bin_groups['12'].append(item)\n",
    "            bin_groups['02'].append(item)\n",
    "    return {bin_labels[k]: bin_groups[k] for k in ('01', '12', '02')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to generate mean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_lmdb_mean(lmdb_path, data_type, reshape_4D = True, plot_mean = True):\n",
    "    import caffe\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    env = initiate_lmdb(lmdb_path, drop_existing = False)\n",
    "    mean = np.empty\n",
    "    i = 0\n",
    "    with env.begin() as txn:\n",
    "        datum = caffe.proto.caffe_pb2.Datum()\n",
    "        cursor = txn.cursor()\n",
    "        cursor.next();\n",
    "        datum.ParseFromString(cursor.value())\n",
    "        mean = np.zeros([datum.channels, datum.height, datum.width])\n",
    "        cursor = txn.cursor()\n",
    "        for key, value in cursor:\n",
    "            i += 1\n",
    "            datum.ParseFromString(value)\n",
    "            flat = np.fromstring(datum.data, dtype=data_type)\n",
    "            x = flat.reshape(datum.channels, datum.height, datum.width)\n",
    "            mean = np.add(mean, x)\n",
    "    mean = np.divide(mean, i)\n",
    "    if plot_mean:\n",
    "        debug_plot_median_slices(mean)\n",
    "    if reshape_4D:\n",
    "        mean = mean.reshape((1,) + mean.shape)\n",
    "        print('mean image reshaped to', mean.shape)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of how to do data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "    'adni_root': '/home/xubiker/ADNI_Multimodal/dataset/',\n",
    "    'max_augm': {'shift': 2, 'blur': 1.2},\n",
    "    'test_prc': 0.25,\n",
    "    'valid_prc': 0.25,\n",
    "    'augm_factor': 2,\n",
    "    'dtype': np.float,\n",
    "\n",
    "    'crop_params': None,#{'shift': (0, 0, -0.05), 'prc': (0.05, 0.05, 0.05)},\n",
    "    'crop_roi_params': (65-2, 92+1-2, 58-2, 85+1-2, 31-2, 58+1-2) # max_shift substracted\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_params(file_path):\n",
    "    import pickle\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate lists..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_lists(lists_file_path, debug=True):\n",
    "    train_list, valid_list, test_list = generate_lists_from_adni2(\n",
    "        params['adni_root'],\n",
    "        params['max_augm'], test_prc=params['test_prc'], valid_prc=params['valid_prc'],\n",
    "        augm_factor=params['augm_factor'],\n",
    "        shuffle_data=True, debug=debug\n",
    "    )\n",
    "    import pickle\n",
    "    with open(lists_file_path, 'wb') as f:\n",
    "        pickle.dump((train_list, valid_list, test_list), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process lists. Write them to lmdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_lmdb(lst, lmdb_name, label_code):\n",
    "    make_lmdb(one_modality_list=lst, adni_root=params['adni_root'], data_type=params['dtype'],\n",
    "              lmdb_name=lmdb_name,\n",
    "              label_code=label_code,\n",
    "              max_augm_shift=params['max_augm']['shift'],\n",
    "              crop_params=params['crop_params'], crop_roi_params=params['crop_roi_params']\n",
    "             )\n",
    "    debug_lmdb_print_info(lmdb_name)\n",
    "    debug_lmdb_plot_slices(lmdb_name, data_type=params['dtype'])\n",
    "    mean = calc_lmdb_mean(lmdb_path=lmdb_name, data_type=params['dtype'], reshape_4D=True, plot_mean=True)\n",
    "    array_to_proto(data=mean, proto_name=lmdb_name+'_mean.binaryproto')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_lmdb_from_lists(lists_file_path, create_binary_lmdbs=False, normalize_labels=False):\n",
    "    \n",
    "    import pickle\n",
    "    with open(lists_file_path, 'rb') as f:\n",
    "        train_list, valid_list, test_list = pickle.load(f)\n",
    "    \n",
    "    train_mri_list, train_dti_list = split_mri_dti(train_list)\n",
    "    valid_mri_list, valid_dti_list = split_mri_dti(valid_list)\n",
    "    test_mri_list, test_dti_list = split_mri_dti(test_list)\n",
    "\n",
    "    lists_with_names = zip(\n",
    "        [train_mri_list, valid_mri_list, test_mri_list, train_dti_list, valid_dti_list, test_dti_list],\n",
    "        ['alz_sMRI_train', 'alz_sMRI_valid', 'alz_sMRI_test', 'alz_MD_train', 'alz_MD_valid', 'alz_MD_test'])\n",
    "#     lists_with_names = zip(\n",
    "#         [test_mri_list],\n",
    "#         ['alz_MRI_test'])\n",
    "\n",
    "    for (lst, name) in lists_with_names:\n",
    "        queue = [(lst, name, label_code_ternary)]\n",
    "        if create_binary_lmdbs:\n",
    "            queue = []\n",
    "            bin_groups = split_lists_to_binary_groups(lst)\n",
    "            for k in bin_groups:\n",
    "                label_code = label_code_binary[k] if normalize_labels else label_code_ternary\n",
    "                queue.append((bin_groups[k], name + '_' + k, label_code))\n",
    "        for (l, n, c) in queue:\n",
    "            generate_lmdb(l, n, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_params('params.pkl')\n",
    "generate_lists('lists.pkl', debug=True)\n",
    "generate_lmdb_from_lists('lists.pkl', create_binary_lmdbs=True, normalize_labels=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
